{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "035ef14d-5ef5-4e78-ae35-62366ecaf6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 10:27:33 WARN Instrumentation: [7e6e3561] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n",
      "+---+-------+------+--------+-----------------+\n",
      "| ID|Feature|Target|Features|       prediction|\n",
      "+---+-------+------+--------+-----------------+\n",
      "|  2|   10.0|  25.0|  [10.0]|             25.0|\n",
      "|  4|   20.0|  35.0|  [20.0]|34.99999999999999|\n",
      "+---+-------+------+--------+-----------------+\n",
      "\n",
      "RMSE :  5.0242958677880805e-15\n",
      "R2 :  1.0\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=42) \n",
    "                                                    \n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')\n",
    "\n",
    "# Predictions\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show()\n",
    "\n",
    "# Evaluation\n",
    "evaluator = RegressionEvaluator(labelCol='Target', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName : 'r2'})\n",
    "\n",
    "print(\"RMSE : \", rmse)\n",
    "print(\"R2 : \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2ad41f-8df0-49bb-b5e6-b2a0526e4636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057929559646,4.087352266612518]\n",
      "Intercept: 11.568912727035112\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset\n",
    "data = [(1, Vectors.dense(2.0, 3.0), 0), (2, Vectors.dense(1.0, 5.0), 1), (3, Vectors.dense(2.5, 4.5), 1), (4, Vectors.dense(3.0, 6.0), 0)]\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d43c06c9-8cef-4525-9315-f716fb3dfbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([5.33333333, 5.33333333]), array([15., 15.])]\n",
      "[5.33333333 5.33333333]\n",
      "[15. 15.]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset\n",
    "data = [(1, Vectors.dense(1.0, 1.0)), (2, Vectors.dense(5.0, 5.0)), (3, Vectors.dense(10.0, 10.0)), (4, Vectors.dense(15.0, 15.0))]\n",
    "columns = ['ID', 'Features']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train KMeans clustering model\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "# Show cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(f'Cluster Centers: {centers}')\n",
    "for c in centers:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef2933d-7d69-402e-8097-674567cf238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 12:54:52 WARN Utils: Your hostname, rayfal resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/25 12:54:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/25 12:54:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----------------+------------+-------------------+------+------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|product_id|        product_name|            category|discounted_price|actual_price|discount_percentage|rating|rating_count|        about_product|             user_id|           user_name|           review_id|        review_title|      review_content|            img_link|        product_link|\n",
      "+----------+--------------------+--------------------+----------------+------------+-------------------+------+------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|B07JW9H4J1|Wayona Nylon Brai...|Computers&Accesso...|           399.0|      1099.0|               64.0|   4.2|        NULL| High Compatibilit...|AG3D6O4STAQKAY2UV...|Manav,Adarsh gupt...|R3HXWT0LRP0NMF,R2...|Satisfied,Chargin...|Looks durable Cha...|https://m.media-a...|https://www.amazo...|\n",
      "|B098NS6PVG|Ambrane Unbreakab...|Computers&Accesso...|           199.0|       349.0|               43.0|   4.0|        NULL| Compatible with a...|AECPFYFQVRUWC3KGN...|ArdKn,Nirbhay kum...|RGIQEG07R9HS2,R1S...|A Good Braided Ca...|I ordered this ca...|https://m.media-a...|https://www.amazo...|\n",
      "|B096MSW6CT|Sounce Fast Phone...|Computers&Accesso...|           199.0|      1899.0|               90.0|   3.9|        NULL|【 Fast Charger& D...|AGU3BBQ2V2DDAMOAK...|Kunal,Himanshu,vi...|R3J3EQQ9TZI5ZJ,R3...|Good speed for ea...|Not quite durable...|https://m.media-a...|https://www.amazo...|\n",
      "|B08HDJ86NZ|boAt Deuce USB 30...|Computers&Accesso...|           329.0|       699.0|               53.0|   4.2|        NULL| The boAt Deuce US...|AEWAZDZZJLQUYVOVG...|Omkar dhale,JD,HE...|R3EEUZKKK9J36I,R3...|Good product,Good...|Good product,long...|https://m.media-a...|https://www.amazo...|\n",
      "|B08CF3B7N1|Portronics Konnec...|Computers&Accesso...|           154.0|       399.0|               61.0|   4.2|        NULL| [CHARGE & SYNC FU...|AE3Q6KSUK5P75D5HF...|rahuls6099,Swasat...|R1BP4L2HH9TFUP,R1...|As good as origin...|Bought this inste...|https://m.media-a...|https://www.amazo...|\n",
      "+----------+--------------------+--------------------+----------------+------------+-------------------+------+------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Homework\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AmazonML\").getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "df = spark.read.csv(\"amazon.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Cleaning numeric columns\n",
    "numeric_cols = [\"discounted_price\", \"actual_price\", \"discount_percentage\", \"rating\", \"rating_count\"]\n",
    "\n",
    "\n",
    "df_clean = df\n",
    "df_clean = df_clean.withColumn(\"discounted_price\", regexp_replace(\"discounted_price\", \"[₹,]\", \"\").cast(\"float\"))\n",
    "df_clean = df_clean.withColumn(\"actual_price\", regexp_replace(\"actual_price\", \"[₹,]\", \"\").cast(\"float\"))\n",
    "df_clean = df_clean.withColumn(\"discount_percentage\", regexp_replace(\"discount_percentage\", \"%\", \"\").cast(\"float\"))\n",
    "df_clean = df_clean.withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "df_clean = df_clean.withColumn(\"rating_count\", col(\"rating_count\").cast(\"float\"))\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9facffa-61a0-4f54-8c34-eae07c58ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6607407407407407\n"
     ]
    }
   ],
   "source": [
    "# Homework (Build a classification model + evaluate perfomance)\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_log = df_clean.select(\"rating\", \"rating_count\").na.drop()\n",
    "df_log = df_log.withColumn(\"Label\", (df_log.rating >= 4).cast(\"int\"))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"rating_count\"],\n",
    "    outputCol=\"Features\"\n",
    ")\n",
    "df_log2 = assembler.transform(df_log)\n",
    "\n",
    "train, test = df_log2.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"Features\", labelCol=\"Label\")\n",
    "model = lr.fit(train)\n",
    "\n",
    "pred = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Label\")\n",
    "\n",
    "print(\"AUC:\", evaluator.evaluate(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df31a851-9fc5-4d35-814a-7341186cc673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Params: LogisticRegressionModel: uid=LogisticRegression_7b1495cf2fe1, numClasses=2, numFeatures=1\n",
      "Best regParam: 0.0\n",
      "Best elasticNetParam: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Homework (Explore hyperparameter tuning using crossvalidation)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0, 0.1, 0.01])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Label\")\n",
    "\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "print(\"Best Model Params:\", cvModel.bestModel)\n",
    "print(\"Best regParam:\", cvModel.bestModel._java_obj.getRegParam())\n",
    "print(\"Best elasticNetParam:\", cvModel.bestModel._java_obj.getElasticNetParam())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
